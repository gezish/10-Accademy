{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Satisfaction Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'creat_engine' from 'sqlalchemy' (C:\\Users\\gezahegne.wondachew\\AppData\\Local\\anaconda3\\lib\\site-packages\\sqlalchemy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m creat_engine\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'creat_engine' from 'sqlalchemy' (C:\\Users\\gezahegne.wondachew\\AppData\\Local\\anaconda3\\lib\\site-packages\\sqlalchemy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "from sqlalchemy import create_engine\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import creat_engine\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"expand_frame_repr\", False)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "from df_outlier import DfOutlier\n",
    "from vis_seaborn import *\n",
    "from vis_plotly import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/my_clean_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_engagements = pd.read_csv(\"../data/user_engagements.csv\")\n",
    "user_engagements.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_experiance = pd.read_csv(\"../data/TellCo_user_experience_data.csv\")\n",
    "user_experiance.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Engagement score to each user. Consider the engagement score as the Euclidean distance between the user data point & the less engaged cluster(use the first clustering for this)(Euclidean Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/TellCo_user_engagement.pkl\", \"rb\") as f:\n",
    "    kmeans1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_engagement = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distance between the centroid and samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_df = user_engagements.set_index('MSISDN/Number')[\n",
    "    ['time_duration', 'Total Data Volume (Bytes)', 'user_sessions']]\n",
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(eng_df)\n",
    "pd.DataFrame(scaled_array).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(scaled_array)\n",
    "pd.DataFrame(data_normalized).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = kmeans1.fit_transform(data_normalized)\n",
    "distance_from_less_engagement = list(\n",
    "    map(lambda x: x[less_engagement], distance))\n",
    "user_engagements['engagement_score'] = distance_from_less_engagement\n",
    "user_engagements.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considering the experience score as the Euclidean distance between the user data point & the worst experienceâ€™s cluster members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/TellCo_user_experiance.pkl\", \"rb\") as f:\n",
    "    kmeans2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_experiance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = user_experiance.set_index('MSISDN/Number')[\n",
    "    ['total_avg_rtt', 'total_avg_tp', 'total_avg_tcp']]\n",
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(exp_df)\n",
    "pd.DataFrame(scaled_array).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(scaled_array)\n",
    "pd.DataFrame(data_normalized).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = user_experiance.set_index('MSISDN/Number')\n",
    "distance = kmeans2.fit_transform(data_normalized)\n",
    "distance_from_worest_experiance = list(\n",
    "    map(lambda x: x[worst_experiance], distance))\n",
    "user_experiance['experience_score'] = distance_from_worest_experiance\n",
    "user_experiance.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the average of both engagement & experience scores as the satisfaction score & report the top 10 satisfied customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_engagement = user_engagements['MSISDN/Number'].values\n",
    "user_id_experiance = user_experiance['MSISDN/Number'].values\n",
    "user_intersection = list(\n",
    "    set(user_id_engagement).intersection(user_id_experiance))\n",
    "user_intersection[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_engagement_df = user_engagements[user_engagements['MSISDN/Number'].isin(\n",
    "    user_intersection)]\n",
    "user_engagement_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_experiance_df = user_experiance[user_experiance['MSISDN/Number'].isin(\n",
    "    user_intersection)]\n",
    "user_experiance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.merge(user_engagement_df, user_experiance_df, on='MSISDN/Number')\n",
    "user_df['satisfaction_score'] = (\n",
    "    user_df['engagement_score'] + user_df['experience_score'])/2\n",
    "user_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_score_df = user_df[['MSISDN/Number', 'engagement_score',\n",
    "                        'experience_score', 'satisfaction_score']]\n",
    "sat_score_df = sat_score_df.set_index('MSISDN/Number')\n",
    "sat_score_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_satisfaction = sat_score_df.sort_values(\n",
    "    'satisfaction_score', ascending=False)\n",
    "sat_top_10 = sorted_by_satisfaction['satisfaction_score'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(sat_top_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a regression model of your choice to predict the satisfaction score of a customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(sat_score_df, 'engagement_score',\n",
    "        'experience_score', 'satisfaction_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see whene expirience score and engament score increase, \n",
    "satisfaction score will also increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sat_score_df[['engagement_score', 'experience_score']]\n",
    "y = sat_score_df[['satisfaction_score']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()\n",
    "model = linear_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients: \\n', model.coef_)\n",
    "print(\"Mean squared error: %.2f\" %\n",
    "      np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "print('Variance score: %.2f' % model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a k-means(k=2) on the engagement & the experience score .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df = user_df[[\n",
    "    'MSISDN/Number', \n",
    "    'engagement_score',\n",
    "    'experience_score']].copy()\n",
    "user_satisfaction_df = user_satisfaction_df.set_index('MSISDN/Number')\n",
    "user_satisfaction_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(user_satisfaction_df)\n",
    "scaled_array\n",
    "pd.DataFrame(scaled_array).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(scaled_array)\n",
    "pd.DataFrame(data_normalized).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(data_normalized)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.insert(0, 'cluster', kmeans.labels_)\n",
    "user_satisfaction_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(user_satisfaction_df, x='engagement_score', y=\"experience_score\",\n",
    "                 color='cluster')\n",
    "Image(pio.to_image(fig, format='png', width=1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.to_csv('../data/TellCo_user_satisfaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the average satisfaction & experience score per cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_satisfaction_df.groupby('cluster').agg(\n",
    "    {'engagement_score': 'sum', 'experience_score': 'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1 has higher Engagement and satisfaction score. \n",
    "Cluster 2 has vert low expirience score but higher engagement score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your final table containing all user id + engagement, experience & satisfaction scores in your local MySQL database. Report a screenshot of a select query output on the exported table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+pymysql://root:2203@localhost/telecom_user_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('writing to the database')\n",
    "    frame = sat_score_df.to_sql(\n",
    "        \"telecom_user_analytics\", con=engine, if_exists='replace')\n",
    "    print('Writing Done!')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error writing to database: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_sql(\"select * from telecom_user_db.telecom_user_analytics\", engine)\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model deployment tracking - deploy the model and monitor your model. Here you can use MlOps tools which can help you to track your modelâ€™s change.  Your model tracking report includes code version, start and end time, source, parameters, metrics(loss convergence) and artifacts or any output file regarding each specific run. (CSV file, screenshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
